{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# List of computers\n",
    "\n",
    "Wikipedia is full of lists. One of the lists you might find yourself reading late on night is the list of computers.\n",
    "\n",
    "[the list](https://en.wikipedia.org/wiki/List_of_home_computers)\n",
    "\n",
    "(If there are others lists you would prefer, we're not sure we want to know.) And as interesting as this list is when on wikipedia, how cool would that list be inside an actual python list?\n",
    "\n",
    "Let's start by importing the libraries for regex and downloading webpages to our computer.\n",
    "\n",
    "Note that we're doing this as an example of regex, not as an example of webscraping. When you find information on the internet that you want locally, you should always first look for an API. If that isn't offered try webscraping using beautifulsoup or selenium (the next topic in this course)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import requests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next up is getting the website locally. We have the URI (https://en.wikipedia.org/wiki/List_of_home_computers) and we will be using this to download the site."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uri = \"https://en.wikipedia.org/wiki/List_of_home_computers\"\n",
    "\n",
    "r = requests.get(uri)\n",
    "print(r.url)\n",
    "print(r.status_code)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have a variable r which is a Response-object. We can investigate this further, looking into finding the actual list, because a website is composed of many parts, most of which we are not interested in: we need to strip the header and footer. This is where your knowledge of webdesign will prove invaluable.\n",
    "\n",
    "First thing we need is the HTML-code. This is stored in the text-property. You could print it, but you won't be able to read it all."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(r.text[:300])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When confronted with this problem, the Chrome browser is a great help. It allows you to \"inspect\" a webpage, showing the code you want to find next to the actual webpage:\n",
    "\n",
    "![](images/2022-03-03-19-49-17.png)\n",
    "\n",
    "This way we know that we are looking for \"```<table class=\"wikitable sortable jquery-tablesorter\">```\". Let's start with some regex that will gives us way to much information and refine that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "found = re.search(r'<table class=\"wikitable sortable jquery-tablesorter\">', r.text)\n",
    "if found:\n",
    "    print(found)\n",
    "else:\n",
    "    print(\"found nothing...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We found nothing? Let's go less refined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "found = re.search(r\"<table.*>\", r.text)\n",
    "if found:\n",
    "    print(found)\n",
    "else:\n",
    "    print(\"found nothing...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We found a table! Let's refine again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "found = re.search(r'<table class=\"wikitable sortable\">', r.text)\n",
    "if found:\n",
    "    print(found)\n",
    "else:\n",
    "    print(\"found nothing...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we're going to get the content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "found = re.search(r'<table class=\"wikitable sortable\">.*</table>/s', r.text)\n",
    "if found:\n",
    "    print(found)\n",
    "else:\n",
    "    print(\"found nothing...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nothing? We know the beginning is there and the end is there, so what is going on?\n",
    "\n",
    "The solution: . matches everything _except_ newlines. We need flags to include the newlines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "found = re.search(r'<table class=\"wikitable sortable\">.*</table>', r.text, flags=re.DOTALL)\n",
    "if found:\n",
    "    print(found)\n",
    "    the_table = found.group()\n",
    "else:\n",
    "    print(\"found nothing...\")\n",
    "\n",
    "print(the_table[:100])\n",
    "print(the_table[-100:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alright, so now we have a variable containing the table we want to investigate. (We stored it in a variable so we don't need to worry about the rest of the page anymore.) We want as much information as we can find. For this we need all the text between \"tr\"-tags. Within those tags we'll be looking for the second, third and fourth cell.\n",
    "\n",
    "![](images/2022-03-03-20-15-45.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "found = re.findall(r\"<tr>.*</tr>\", the_table, flags=re.DOTALL)\n",
    "\n",
    "if not found:\n",
    "    raise Exception(\"Nothing found. Shouldn't be happening.\")\n",
    "\n",
    "the_list = found\n",
    "\n",
    "print(the_list[0])\n",
    "print(the_list[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We found the first item in the list well enough, but there doesn't seem to be a second item. Which is odd. But remember: * is greedy. All items in the table are gathered in the first item. That is not what we want. We want [non-greedy](https://docs.python.org/3/howto/regex.html#greedy-versus-non-greedy)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "found = re.findall(r\"<tr>.*?</tr>\", the_table, flags=re.DOTALL)\n",
    "\n",
    "if not found:\n",
    "    raise Exception(\"Nothing found. Shouldn't be happening.\")\n",
    "\n",
    "the_list = found\n",
    "\n",
    "print(the_list[0])\n",
    "print(the_list[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the list of computers stored in the_list (and you should really use better variable names, we're totally providing this as an example of laziness and not actual laziness) we can go over this list and regex the different fields out of it. We'll start out by getting the column headers (in the_list[0]), next we'll start all separate fields in a list of dictionaries.\n",
    "\n",
    "Do note: the headers are slightly different, as Chrome Inspect tells us:\n",
    "\n",
    "![](images/2022-03-03-21-02-57.png)\n",
    "\n",
    "(Also: did you note how we went from \"if found...\" to \"if not found...\"? Because we expect stuff to be found this is a way more concise method of writing code. In future codeblocks we'll drop even that and simply deal with the errors if nothing is found.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = the_list[0]\n",
    "found = re.findall(\"<th.*?>(.*?)</th>\", headers, flags=re.DOTALL)\n",
    "\n",
    "for f in found:\n",
    "    print(\"_\", f, \"_\", sep=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Good, but we can do better. There's a newline after every line that we don't want and using the non-greedy operator to get rid of the text within the opening tag is also pretty lazy. Both can be fixed by writing better regexes.\n",
    "\n",
    "There's also the line with \"\\<br /\\>...\". Can we get rid of that text as well?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "found = re.findall(\"<th[^>]*>(.*?)\\n?<[br|/th]\", headers, flags=re.DOTALL)\n",
    "\n",
    "for f in found:\n",
    "    print(\"_\", f, \"_\", sep=\"\")\n",
    "\n",
    "table_titles = found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "found = re.findall(r\"<td>(.*?)\\n?</td>\", the_list[1], flags=re.DOTALL)\n",
    "\n",
    "for f in found:\n",
    "    print(\"_\", f, \"_\", sep=\"\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Done. You could figure out a way of cutting all the links, but the links are actually pretty interesting. If you want to go that way, try [regex101](https://regex101.com), set it to \"Python\" and paste some raw text below. That way you'll have a very easy way of tinkering with the regex.\n",
    "\n",
    "![](images/2022-03-03-21-29-35.png)\n",
    "\n",
    "But keeping the links we only have to go over the entire list and create our list of dictionaries!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_list = []\n",
    "\n",
    "for item in the_list[1:]:\n",
    "    found = re.findall(r\"<td>(.*?)\\n?</td>\", the_list[1], flags=re.DOTALL)\n",
    "\n",
    "    dict = {}\n",
    "    for index, text in enumerate(found):\n",
    "        dict[table_titles[index]] = text\n",
    "    output_list.append(dict)\n",
    "\n",
    "print(output_list[7][\"Model\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So that was a simple example. Simple because we used wikipedia: wikipedia prouds itself in creating nice and structured pages. Suppose you wanted to know all the records of \"the heaviest list\", the list of 666 heavy metal records that StuBru broadcasts every year around easter, then you'd have a much bigger problem. Check the following screenshot:\n",
    "\n",
    "![](images/2022-03-02-15-47-09.png)\n",
    "\n",
    "Notice the \"loading\" in the center? This means the list is generated by using JavaScript after the page has loaded. Therefore the request we made in the beginning won't contain the list. You can solve this using better screenscraping methods which, incidentally, are the topic of the next couple of notebooks."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.2 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e31c2e8d3ed3a4908bbc2c2b66173ad7e7558239e8d6f52669fbf04aeb9634e1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
